#Default LLAVA Model uses more RAM
#LLAVA_MODEL_PATH = '/workspace/SUPIR/models/LLaVA v1.5 13B/llava-v1.5-13b'
#Secondary LLAVA Model uses less RAM
LLAVA_MODEL_PATH = '/workspace/SUPIR/models/LLaVA v1.5 7B/llava-v1.5-7b'
LLAVA_CLIP_PATH = '/workspace/SUPIR/models/LLaVA CLIP/clip-vit-large-patch14-336'
SDXL_CLIP1_PATH = '/workspace/SUPIR/models/SDXL CLIP Encoder-1/clip-vit-large-patch14'
SDXL_CLIP2_CKPT_PTH = '/workspace/SUPIR/models/SDXL CLIP Encoder-2/CLIP-ViT-bigG-14-laion2B-39B-b160k/open_clip_pytorch_model.bin'
